{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROCESS A TECHNICAL BUS RFP INTO A COMPLIANCE MATRIX**\n",
    "\n",
    "**File Name**: 190905_RFP_Processor.ipynb\n",
    "\n",
    "**Inputs:** \n",
    "- Excel document with these tabs: meta, toc and technical\n",
    "- Compliance Matrix Bank\n",
    "\n",
    "**Outputs:** Compliance Matrix with compliance indications and PAE Suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# ESTABLISMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTERNAL FILES\n",
    "import glob\n",
    "# import pickle\n",
    "import os\n",
    "import requests # was use to handle proxies\n",
    "\n",
    "import sys\n",
    "# module_path = os.path.abspath(os.path.join('..'))\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "# from D3AI_A7_utils import AIA7_190000_Module as utils\n",
    "\n",
    "# DEFAULT DATA SCICENCE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "# PLOTING\n",
    "# Matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# import seaborn as sn\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.preprocessing.text as Tokenizer\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "from typing import List\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import nltk\n",
    "\n",
    "import gensim\n",
    "import collections\n",
    "import smart_open\n",
    "import collections\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import warnings\n",
    "\n",
    "# import tabula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timex(startTm):\n",
    "    print(f'Duration: {time.time() - startTm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeStartPipe = time.time()\n",
    "\n",
    "# COMMON:\n",
    "maxlen = 100 # Max number of words in description\n",
    "embedding_size = 300\n",
    "\n",
    "\n",
    "# DEVELOPMENT:\n",
    "# Transactional / Parser\n",
    "# rfp_path = '../../nb_bid_data/190909_bid_pipe/input/'\n",
    "# output_path = '../../nb_bid_data/190909_bid_pipe/output/'\n",
    "rfp_path = ''\n",
    "output_path = ''\n",
    "# Keras Compliance Classifier\n",
    "keras_tokenizer_path = '../../nb_bid_data/190909_bid_pipe/190927V3_keras_tokenizer.txt'\n",
    "keras_classification_model_path = '../../nb_bid_data/190909_bid_pipe/190927V3_simple_nlp_keras_model.h5'\n",
    "# Smooth Inverse Frequency\n",
    "word2vec_path ='../../nb_bid_data/190909_similarity/190927_gensim_word2vec_embedding_model.bin'\n",
    "bank_path = '../../nb_bid_data/190909_bid_pipe/190903_all_4_bid_compilations_dl.xlsx'\n",
    "\n",
    "\n",
    "# # EXECUTABLE:\n",
    "# # Transactional / Parser\n",
    "# rfp_path = ''\n",
    "# output_path = ''\n",
    "# # Keras Compliance Classifier\n",
    "# keras_tokenizer_path = '190927V3_keras_tokenizer.txt'\n",
    "# keras_classification_model_path = '190927V3_simple_nlp_keras_model.h5'\n",
    "# # Smooth Inverse Frequency\n",
    "# word2vec_path ='190927_gensim_word2vec_embedding_model.bin'\n",
    "# bank_path = '190903_all_4_bid_compilations_dl.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# FORMULAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Transactional Formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Formulas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### format_headings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_headings(ls):\n",
    "    ls = [x.lower() for x in ls]\n",
    "    ls = [x.strip() for x in ls]\n",
    "    ls = [re.sub(\" \", \"_\", x) for x in ls]\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_standard_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maid df\n",
    "def create_standard_df():\n",
    "    '''\n",
    "    Creating an empty df to be use as the standard df to be pass in the pipes\n",
    "    '''\n",
    "    col_names = ['CID','HEADING','LIST','SPEC','TABLE COL 1','TABLE COL 2','TABLE COL 3','TABLE COL 4','TABLE COL 5','TABLE COL 6','TABLE COL 7',\n",
    "                 'TABLE COL 8','TABLE COL 9','BLANK','BLACKLIST','BRAND','SECTION','SUB-SECTION','PAGE',\n",
    "                 \n",
    "                 'SALES ENG GROUP','AI SE COMPLIANCE','ACTION','TAG','NUMBER','MODEL','AI MODEL','AI VARIANT','PAE','SE COMMENT','SE FEEDBACK TO IT',\n",
    "                 'AI PAE 1','AI PAE 1 SPEC','AI PAE 1 SOURCE','AI PAE 2','AI PAE 2 SPEC','AI PAE 2 SOURCE','AI PAE 3','AI PAE 3 SPEC','AI PAE 3 SOURCE',\n",
    "                 'RATIONAL','PD REQUEST','PD COMMENT','AGENCY RESPONSE','AGENCY COMMENT/ADDENDUM',\n",
    "                 \n",
    "                 'PLC GROUP','AI ENG COMPLIANCE','PLC STATUS','PLC COMMENT',\n",
    "                 'ENG OWNER','ENG COMPLIANT','ENG DATE','PROGRAM','REVIEW SIGNATURE','MEANS OF COMPLIANCE','ARGUMENT','INTERNAL COMMENT','PLC FEEDBACK TO IT',\n",
    "                 \n",
    "                 'PIT GROUP','ORG AI SE COMPLIANCE','ORG AI BLACKLIST','ORG AI BRAND','ORG AI TAG','ORG AI SECTION','ORG AI SUB-SECTION',\n",
    "                 'ORG AI PAE 1 ID','ORG AI PAE 1 SCORE','ORG AI PAE 2 ID','ORG AI PAE 2 SCORE','ORG AI PAE 3 ID','ORG AI PAE 3 SCORE',\n",
    "                 'ORG AI PD MEANS OF COMPLIANCE','ORG AI PD ARGUMENT','CAT','SPEC_ONE_COL','WORD NBR','ORG COL NBR','RFP NUMBER',\n",
    "                 'RFP NAME','CLIENT NUMBER','ID']\n",
    "    ls = format_headings(col_names)\n",
    "    df = pd.DataFrame(columns=ls)\n",
    "    df.head(2)\n",
    "    print(\"     Created main dataframe.\")\n",
    "    return df, ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_rfp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rfp(rfp_path):\n",
    "    # Load all Word documents in the repo\n",
    "    file_names = [{'path':pathTx} for pathTx in glob.glob(f'{rfp_path}*.xlsx')]\n",
    "    # Eliminate temp files\n",
    "    file_names = [bid for bid in file_names \n",
    "                if not next(iter(re.findall(r'~', bid['path'], flags=re.IGNORECASE)), None)]\n",
    "    print(f'     Number of documents found: {len(file_names)} document(s)')\n",
    "    # Documents to analyse\n",
    "    print(f'     Processing document documents: {file_names}')\n",
    "    file_names[0]['path']\n",
    "\n",
    "    # Loading each line of the source file into a list\n",
    "    # Currently only processing the first file\n",
    "    print('     Loading the RFP...')\n",
    "    meta = pd.read_excel(file_names[0]['path'], sheet_name='meta', header=None)\n",
    "    toc = pd.read_excel(file_names[0]['path'], sheet_name='toc', header=None)\n",
    "    tech = pd.read_excel(file_names[0]['path'], sheet_name='technical', header=None)\n",
    "    print('     ...RFP loaded')\n",
    "    return meta, toc, tech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_ref_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_ref_files(keras_classification_model_path):\n",
    "#     conformity_model = keras.models.load_model(keras_classification_model_path)\n",
    "\n",
    "#     return conformity_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "**Output Formulas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### formating_out_cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formating_out_cid(df):\n",
    "    se = df.cid.str.replace(r'^\\s*$', \"\", regex=True)\n",
    "    mask = se.str.len() < 1\n",
    "    df.loc[mask, 'cid'] = np.nan\n",
    "    mask2 = df.cat.str.contains(r'para|list|table')\n",
    "    df.loc[mask2,'cid'] = df.loc[mask2,'cid'].fillna(method='ffill')\n",
    "    df.cid = df.cid.fillna(value=\"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt = formating_out_cid(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask\\ = df.cat.str.contains(r'', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df.cid.fillna(method='ffill')\n",
    "# df.cid.ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### formating_tc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formating_out_tc(main):\n",
    "    main['spec_one_col'] = main.spec\n",
    "    mask = main.cat == 'table'\n",
    "#     mask = \":\"\n",
    "    split_df = main.loc[:, 'spec'].str.split(pat='<tc>',expand=True)\n",
    "    main.loc[:,'spec_one_col'] = main.loc[:,'spec'].str.replace(r'<tc>',\" \")\n",
    "#     breakpoint()\n",
    "    for col in split_df:\n",
    "        if col == 0:\n",
    "            main.loc[mask,'spec'] = split_df[col]\n",
    "        else:\n",
    "            main.loc[mask,f'table_col_{col}'] = split_df[col]\n",
    "\n",
    "    \n",
    "#     for col in split_df:\n",
    "# #         breakpoint()\n",
    "#         if (col - spec_first_col) < 0:\n",
    "#             breakpoint()\n",
    "#             main.loc[:,'cid'] = main.loc[:,'cid'] + split_df[col]\n",
    "#         elif (col - spec_first_col) == 0:\n",
    "# #             breakpoint()\n",
    "#             main.loc[:,'spec'] = split_df[col]\n",
    "#         else:\n",
    "# #             breakpoint()\n",
    "#             main.loc[:,f'table_col_{col-spec_first_col}'] = split_df[col]\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('cat').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt = formating_tc(df, spec_first_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt.tail(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### formating_heading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formating_heading(main):\n",
    "    '''\n",
    "    Move the headings in on distinc column\n",
    "    Forward fill the headings\n",
    "    '''\n",
    "#     breakpoint()\n",
    "    mask = main.cat == 'heading'\n",
    "    main.loc[mask, 'heading'] = main.loc[mask, 'spec']\n",
    "    main.loc[mask, 'spec'] = np.nan\n",
    "    main.heading = main.heading.ffill()\n",
    "#     main['admin_group'] = 'eof'\n",
    "    return main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_df(df, rejected, columns_names_ls, output_path, df_file_name_prefix=None):\n",
    "    df['id'] = df.index.values\n",
    "    # Only keeping the standard format\n",
    "    df = df[columns_names_ls]\n",
    "    \n",
    "    full_path = f'{output_path}{bid_number}_{bid_name}_output_raw.xlsx'\n",
    "    writer = pd.ExcelWriter(full_path, engine='xlsxwriter')\n",
    "    try:\n",
    "        df.to_excel(writer, na_rep=\"\", header=False, sheet_name='main', index=None)\n",
    "#         rejected_full_path = f'{output_path}{df_file_name_prefix}_{bid_number}_{bid_name}_rejected_output.xlsx'\n",
    "        if rejected.shape[0] > 0:\n",
    "            rejected.to_excel(writer, na_rep=\"\", header=True, sheet_name='rejected')\n",
    "        writer.save()\n",
    "        print(f'Document successfuly created!')\n",
    "    except:\n",
    "        print(f'ERROR: Document was NOT saved! Is the file currenlty open?')\n",
    "\n",
    "    return _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Parsing Formulas\n",
    "These parsing formulas are conceived to be ran consecutively as per the below order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse_raw_row_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_raw_row_column(main, column_name, raw):\n",
    "#     '''\n",
    "#     INPUTS:\n",
    "#         main = Conformity df\n",
    "#         columns_name = Name of the targeted feature\n",
    "#         scramble = df created from an Acrobat PDF converted to an excel spreadsheet that need to be parsed\n",
    "    \n",
    "#     OUTPUT: Main df\n",
    "    \n",
    "#     Take a df created from an Acrobat PDF converted to an excel spreadsheet.\n",
    "#     Convert that df into a list that become the rows of the df\n",
    "#     Identify each cell beyond the first column as a table cell\n",
    "#     '''\n",
    "#     print(\"     Identifying lines and paragraphs...\")\n",
    "\n",
    "#     # Replace unrocognized caracters\n",
    "#     raw = raw.replace({'\\\\uf0a7(?=\\s)':'-'}, regex=True)\n",
    "    \n",
    "#     spec = []\n",
    "#     cat = []\n",
    "#     for index, row in raw.iterrows():\n",
    "#         row_list = []\n",
    "#         # Grouping all columns of the current row in one list\n",
    "#         for col in row:\n",
    "#             # testing of the value of the column is not np.nan\n",
    "#             if not (col != col):\n",
    "#                 row_list.append(col)\n",
    "#         # if True, then it is a row of a table\n",
    "#         if len(row_list) > 1:\n",
    "#             text = \"<tc>\".join(str(x).strip() for x in row_list)\n",
    "#             spec.append(text)\n",
    "#         # if True then it is not a row of a table\n",
    "#         # and it should be splited for in seperate spec is their is '\\n'\n",
    "#         elif len(row_list) == 1:\n",
    "#             row_list = str(row_list[0]).split('\\n')\n",
    "# #             print(row_list)\n",
    "#             spec = spec + row_list\n",
    "#     main[column_name] = pd.Series(spec).astype(str)    \n",
    "#     print(f'          ...Found {len(spec)} paragraphs') \n",
    "#     return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_row_column(main, column_name, raw, spec_first_col):\n",
    "    '''\n",
    "    INPUTS:\n",
    "        main = Conformity df\n",
    "        columns_name = Name of the targeted feature\n",
    "        scramble = df created from an Acrobat PDF converted to an excel spreadsheet that need to be parsed\n",
    "    \n",
    "    OUTPUT: Main df\n",
    "    \n",
    "    Take a df created from an Acrobat PDF converted to an excel spreadsheet.\n",
    "    Convert that df into a list that become the rows of the df\n",
    "    Identify each cell at spec_first_col position and beyond as a table cell\n",
    "    '''\n",
    "    print(\"     Identifying lines and paragraphs...\")\n",
    "\n",
    "    # Replace unrocognized caracters\n",
    "    raw = raw.replace({'\\\\uf0a7(?=\\s)':'-'}, regex=True)\n",
    "    cid_ls = []\n",
    "    spec_ls = []\n",
    "    for index, row in raw.iterrows():\n",
    "        row_list = []\n",
    "        \n",
    "        # Grouping all columns of the current row in one list\n",
    "        for col in row:\n",
    "            # testing of the value of the column is not np.nan\n",
    "            if not (col != col):\n",
    "                row_list.append(col)\n",
    "                \n",
    "        # if True, then it is a row of a table\n",
    "        if len(row_list) > 1:\n",
    "            cid = \" \".join(str(x).strip() for x in row_list[:spec_first_col])\n",
    "            text = \"<tc>\".join(str(x).strip() for x in row_list[spec_first_col:])\n",
    "            cid_ls.append(cid)\n",
    "            spec_ls.append(text)\n",
    "\n",
    "        # if True then it is not a row of a table\n",
    "        # and it should be splited for in seperate spec if their is '\\n'\n",
    "        elif len(row_list) == 1:            \n",
    "            row_list = str(row_list[0]).split('\\n')\n",
    "            cid_ls = cid_ls + [\" \" for x in row_list]\n",
    "            spec_ls = spec_ls + row_list\n",
    "    \n",
    "    main[column_name] = pd.Series(spec_ls).astype(str)    \n",
    "    main['cid'] = pd.Series(cid_ls).astype(str)\n",
    "    \n",
    "    print(f'          ...Found {len(spec_ls)} paragraphs') \n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt = parse_raw_row_column(df,'spec', tech, spec_first_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt.loc[100:150,'cid':'spec']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_string(main, column_name):\n",
    "    print('     Parsing text...')\n",
    "    main[column_name] = main[column_name].str.replace(u'\\xa0', u' ')\n",
    "    main[column_name] = main[column_name].str.replace(u\"\\'\", u\"'\")\n",
    "    main[column_name] = main[column_name].str.replace(r'\\s{2,}', u\" \")\n",
    "    main[column_name] = main[column_name].str.strip()\n",
    "    print(f'          ...Parsed: {main[column_name].size} entries')\n",
    "    return main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse_toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_toc(main, toc):\n",
    "    '''\n",
    "    toc: This is the df with the table of content\n",
    "    Identify the headings in the main document from the toc df\n",
    "    '''\n",
    "    print('     Identifying the headings...')\n",
    "    temp = pd.DataFrame(columns=['toc','text','page'])\n",
    "    # The heading mayby in several columns\n",
    "    temp = parse_raw_row_column(temp, 'toc', toc, 0)\n",
    "    temp = parse_string(temp, 'toc')\n",
    "#     breakpoint()\n",
    "    # replacing the \"....\" of the spaces between the heading and its page number\n",
    "    temp['toc'] = temp['toc'].str.replace(r'\\.{3,}',\" <toc> \")\n",
    "    temp['toc'] = temp['toc'].str.replace(r'\\s{6,}',\" <toc> \")\n",
    "\n",
    "    mask = temp['toc'].str.contains('<toc>')\n",
    "    temp.loc[~mask,'toc'] =  + temp.loc[~mask,'toc'] + '<toc>'\n",
    "\n",
    "    # extracting the headings and the page numbers\n",
    "    temp['text'] = temp['toc'].str.extract(r'(^.*)(?<=<toc>)')\n",
    "    temp['page'] = temp['toc'].str.extract(r'(?=<toc>)(.+$)') \n",
    "\n",
    "    # removing the <toc> anotation added at line 11\n",
    "    temp['text'] = temp['text'].str.replace(r'<toc>', \"\")\n",
    "    temp['page'] = temp['page'].str.replace(r'<toc>', \"\")\n",
    "\n",
    "    temp = parse_string(temp, 'text')\n",
    "    \n",
    "    temp = count_se_word(temp, 'toc', 'word_nbr')\n",
    "    \n",
    "    # also extracting heading text for cases where heading number was put into the CID fields\n",
    "#     temp2 = temp\n",
    "#     temp2.text = temp2.str.extract \n",
    "    \n",
    "#     breakpoint()\n",
    "    print(f'     TOC contains {temp.shape[0]} headings')\n",
    "    # adding the information on the main df\n",
    "    for index, row in temp.iterrows():\n",
    "        mask2 = main.spec.str.startswith(str(row.text))\n",
    "        # Heading canditate should be less than 20 words\n",
    "#         breakpoint()\n",
    "        if int(row['word_nbr']) < 20 and (mask2 == True).any():\n",
    "            idx = main.loc[mask2,'word_nbr'].idxmin()\n",
    "            main.loc[idx,'cat'] = 'heading'\n",
    "            main.loc[idx,'page'] = row.page    \n",
    "    headings_nbr = main[main['cat'] == 'heading'].shape[0]\n",
    "    print(f'          ...Matched {headings_nbr} headings from the main document against the TOC')    \n",
    "    return main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = parse_toc(df, toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_list(main):\n",
    "    print('     Identifiying lists item...')\n",
    "    mask = main['spec'].str.match(r'^•(?=\\s)|^(?=\\s)|^-(?=\\s)|^[A-Za-z][\\.\\)](?=\\s)|^[0-9]{1,3}[\\.\\)](?=\\s)')\n",
    "    main.loc[mask,'cat'] = 'list'\n",
    "    main.loc[mask,'list'] = main.loc[mask,'spec'].str.extract(r'(^•(?=\\s)|^(?=\\s)|^-(?=\\s)|^[A-Za-z][\\.\\)](?=\\s)|^[0-9]{1,3}[\\.\\)](?=\\s))')[0]\n",
    "    main.loc[mask,'list'] = main.loc[mask,'list'].str.replace(r'^',\"-\")\n",
    "    main.list = main.list.str.strip()\n",
    "    main.loc[mask,'spec'] = main.loc[mask,'spec'].str.replace(r'^•(?=\\s)|^(?=\\s)|^-(?=\\s)|^[A-Za-z][\\.\\)](?=\\s)|^[0-9]{1,3}[\\.\\)](?=\\s)',\"\")\n",
    "    main.spec = main.spec.str.strip()\n",
    "    print(f'          ...Found {main[mask].shape[0]} list items')\n",
    "    return main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_table(main, col_name, spec_first_col):\n",
    "    ''' \n",
    "    Classifying the \"cat\" attribute as table for table.\n",
    "    '''\n",
    "    print('     Identifiying tables...')\n",
    "#     mask = main[col_name].str.contains(r'<tc\\d{1,2}>')\n",
    "    main.org_col_nbr = main[col_name].str.count('<tc>')\n",
    "#     breakpoint()\n",
    "    mask = main.org_col_nbr > spec_first_col\n",
    "    main.loc[mask,'cat'] = 'table'\n",
    "    print(f'          ...Found {main[mask].shape[0]} table rows')\n",
    "    return main            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sentence are cut (i.e. PDF page break...) merge back together\n",
    "# def parse_page(main):\n",
    "#     print(f'     Correcting pages breaks...')                    \n",
    "#     maxIter = main.shape[0] -1\n",
    "#     main['cat'] = main['cat'].fillna('para')\n",
    "#     new_df = main[0:0]\n",
    "#     # The for loops need consecutive index\n",
    "#     main = main.reset_index(drop=True)\n",
    "#     skip = False\n",
    "#     i = 0\n",
    "#     for index, row in main.iterrows():\n",
    "#         # Checking current line\n",
    "#         if not re.search(r'\\.$|\\?$|\\!$|\\$:', row.spec) and row['cat'] == 'para' and index < maxIter and skip == False:\n",
    "#             # Checking next line                \n",
    "#             if main.loc[index+1,'cat'] == 'para' and not re.search(r'^[A-Z]', main.loc[index+1,'spec']):\n",
    "# #             and not re.search(r'^[A-Z,a-z]{1,3}.\\d{1,3}.\\d{1,3}\\.?\\s{1,3}\\w*|^\\d{1,3}.\\d{1,3}\\.?\\s{1,3}\\w*', main.loc[index+1,'spec']):\n",
    "#                 # Merging current row and next row specs\n",
    "#                 main.loc[index,'spec'] = str(main.loc[index,'spec']) +\" \"+str(main.loc[index+1,'spec'])\n",
    "#                 skip = True\n",
    "#                 main.loc[index,'dlt'] = False\n",
    "#                 i += 1\n",
    "#             # If skip is true, it was set true at the last operation\n",
    "#             # Do noting with the spec and reset to skip to false\n",
    "#             else:\n",
    "#                 main.loc[index,'dlt'] = False\n",
    "#         elif skip == True:\n",
    "#             skip = False\n",
    "#             main.loc[index,'dlt'] = True\n",
    "#         else:\n",
    "#             main.loc[index,'dlt'] = False\n",
    "#     dlt = main[main['dlt'] == True]\n",
    "#     # Deleting extrat columns\n",
    "#     main = main.loc[main['dlt'] == False,:'id']\n",
    "#     print(f'          ...Number of pages breaks corrected: {i}')\n",
    "#     return main, dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence are cut (i.e. PDF page break...) merge back together\n",
    "def parse_page(main):\n",
    "    print(f'     Correcting pages breaks...')                    \n",
    "    maxIter = main.shape[0]\n",
    "    main['cat'] = main['cat'].fillna('para')\n",
    "#     new_df = main[0:0]\n",
    "    # The for loops need consecutive index\n",
    "    main = main.reset_index(drop=True)\n",
    "    main['dlt'] = False\n",
    "    i = 0\n",
    "    j = 0\n",
    "    k = 0\n",
    "    itr = False\n",
    "    while True:\n",
    "        if not re.search(r'\\.$|\\?$|\\!$|\\$:', main.loc[i,'spec']) and main.loc[i,'cat'] == 'para':\n",
    "            if main.loc[i+1,'cat'] == 'para' and not re.search(r'^[A-Z]', main.loc[i+1,'spec']):\n",
    "                main.loc[i,'spec'] = str(main.loc[i,'spec']) +\" \"+str(main.loc[i+1,'spec'])\n",
    "                main.loc[i+1,'dlt'] = True\n",
    "                itr = True\n",
    "                k += 1\n",
    "                \n",
    "        # NLTK Sentence Tokenizer Merge\n",
    "#         if not re.search(r'\\.$|\\?$|\\!$|\\$:', main.loc[i,'spec']) and main.loc[i,'cat'] == 'para':                            \n",
    "#             if main.loc[i+1,'cat'] == 'para' and not re.search(r'^[A-Z]', main.loc[i+1,'spec']):\n",
    "#                 main.loc[i,'spec'] = str(main.loc[i,'spec']) +\" \"+str(main.loc[i+1,'spec'])\n",
    "#                 main.loc[i+1,'dlt'] = True\n",
    "#                 itr = True\n",
    "#                 k += 1\n",
    "        j += 1\n",
    "        i += 1\n",
    "        if i >= maxIter:\n",
    "#             print(j)\n",
    "            if itr == False:\n",
    "                break\n",
    "            else:\n",
    "                itr = False\n",
    "            i = 0\n",
    "        main = main.loc[main['dlt'] == False,:]\n",
    "        main = main.reset_index(drop=True)\n",
    "        maxIter = main.shape[0]\n",
    "    print(f'          ...Number of pages breaks corrected: {k} in {j/i} loop(s)')\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt = parse_page(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gg, dlt = tt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gg.loc[18,'spec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gg.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse_heading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_heading(df):\n",
    "    '''\n",
    "    Look for matching heading patterns for heading not in the TOC\n",
    "    '''\n",
    "    print(f'     Looking for heading not captured by TOC...')\n",
    "    mask = df.spec.str.contains(r'^\\w{1,3}.\\d{1,3}.*') & (df.word_nbr < 13) & df.cat.str.contains('para') \n",
    "#     mask = df.cat.str.contains('para')\n",
    "    df.loc[mask,'cat'] = 'heading'\n",
    "#     breakpoint()\n",
    "    print(f'     ... found {df[mask].shape[0]} headings not captured by TOC.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('cat').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_heading(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse_paragraph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_paragraph(main):\n",
    "    '''\n",
    "    Parsing large paragraphs to paragraphs with less than 75 words\n",
    "    when possible (require the paragraph to have more than 1 sentence to be split).\n",
    "    '''\n",
    "    print(f'     Current document has {main.shape[0]} rows. Now spliting long paragraphs...')\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        fxn()\n",
    "    #     main['cat'] = main['cat'].fillna('para')\n",
    "        new_ls = main[['cid','list','spec','cat','page']].values.tolist()\n",
    "        itr = True\n",
    "        itr_counter = 0\n",
    "        split_counter = 0\n",
    "        re_split = re.compile(r'\\.\\s{1,3}(?=[A-Z])')\n",
    "        while itr and itr_counter < 10:\n",
    "            itr_counter += 1\n",
    "            current_ls = new_ls\n",
    "            new_ls = []\n",
    "            itr = False\n",
    "            for idx, ls in enumerate(current_ls):\n",
    "                try:\n",
    "                    words_nbr = count_word(ls[2])\n",
    "                except:\n",
    "                    breakpoint()\n",
    "                # count number of sentences\n",
    "                sentences = re_split.split(ls[2])\n",
    "                sentences_nbr = len(sentences) if isinstance(sentences, list) else 0\n",
    "                # large paragraphs are split in the middle\n",
    "                if (ls[3] == 'para' or ls[3] == 'list') and words_nbr > 75 and sentences_nbr > 1:\n",
    "                    split_counter += 1\n",
    "                    middle = find_list_middle(sentences)\n",
    "                    itr = True\n",
    "                    # Split the spec into to spec in the middle.\n",
    "                    new_ls.append([ls[0],ls[1], '. '.join(sentences[:middle])+'.', ls[3], ls[4]])\n",
    "                    new_ls.append([ls[0],ls[1], '. '.join(sentences[middle:]), ls[3], ls[4]])\n",
    "                else:\n",
    "                    new_ls.append(ls)\n",
    "            temp = pd.DataFrame(new_ls, columns=['cid','list','spec','cat','page'])\n",
    "            main = main.iloc[0:0]\n",
    "            main.cid = temp.cid\n",
    "            main.list = temp.list\n",
    "            main.spec = temp.spec\n",
    "            main.cat = temp.cat\n",
    "            main.page = temp.page\n",
    "        print(f'          ...Splited {split_counter} paragraphs over {itr_counter} document cycles. Document has now {main.shape[0]} rows. ')\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt = parse_paragraph(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse_out_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_out_text(df):\n",
    "    '''\n",
    "    Removing data that should not make it into the Compliance Matrix\n",
    "    '''\n",
    "    print('     Removing unwanted text (i.e. page numbers in the footers)...')\n",
    "\n",
    "    # Match \"Page XX of XX\" and other variants of this\n",
    "    # Remove spec with no text\n",
    "    mask = (df.spec.str.contains(r'^Page\\s{1,3}\\d{1,4}\\sof\\s{1,3}\\d{1,4}$|^Page\\s{1,3}\\d{1,4}$|^p.\\s{1,3}\\d{1,4}$', flags=re.IGNORECASE)) | (df.word_nbr == 0)\n",
    "    if (mask == True).any():\n",
    "        rejected = df[mask]\n",
    "        df = df[~mask]\n",
    "    else:\n",
    "        rejected = df[0:0]\n",
    "    \n",
    "    print(f'          ...Rejected {rejected.shape[0]} entries.')\n",
    "    return df, rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_out_text(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Features Formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "    print(\"     Tokenize specs...\")\n",
    "#     breakpoint()\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            tt = sent_tokenize(row.spec)[0]\n",
    "        except:\n",
    "            breakpoint()\n",
    "    df['spec_tokens'] = df.spec.apply(lambda x: sent_tokenize(str(x))[0])\n",
    "    df['spec_tokens'] = df.spec_tokens.str.lower()\n",
    "    df['spec_tokens'] = df.spec_tokens.str.replace(r'\\.\\!\\:\\?', \"\")\n",
    "    # split into words\n",
    "    df['spec_tokens'] = df.spec_tokens.apply(lambda x: word_tokenize(str(x)))\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    df['spec_tokens'] = df.spec_tokens.apply(lambda ls: [token for token in ls if len(token) > 1])\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df.spec_tokens = df.spec_tokens.apply(lambda ls: [w for w in ls if not w in stop_words])\n",
    "    print(\"    ...Filtered stop words.\")\n",
    "    # stemming of words\n",
    "    porter = PorterStemmer()\n",
    "    df['spec_tokens'] = df.spec_tokens.apply(lambda ls: [porter.stem(token) for token in ls])\n",
    "    print(\"    ...Stemmed word.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = tokenize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add_meta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_meta(main, meta):\n",
    "    print('     Collecting RFP information...')\n",
    "#     breakpoint()\n",
    "    spec_first_col = meta.loc[meta[0] == 'Spec First Column',1].iloc[0]\n",
    "    number = meta.loc[meta[0] == 'RFP Number',1].iloc[0]\n",
    "    name = meta.loc[meta[0] == 'RFP Name',1].iloc[0]\n",
    "#     breakpoint()\n",
    "    main['rfp_number'] = number\n",
    "    main['rfp_name'] = name\n",
    "#     df['eng_date'] = str(number)[:2]\n",
    "    print('          ...Project is: ',number)\n",
    "    print('          ...Project Name is: ',name)\n",
    "    return main, number, name, spec_first_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df, bid_number, bid_name, spec_first_col = add_meta(df, meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_id(main):\n",
    "    print('     Creating specs ids...')\n",
    "    main.reset_index(drop=True)\n",
    "#     main['id'] = main.rfp_number + main.index/10000\n",
    "#     main = main.set_index('id')\n",
    "\n",
    "    main.id = main.index\n",
    "    print(f'          ...Created {main.shape[0]} ids.')\n",
    "    return main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Qualifying Formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count_se_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_se_word(main, col_name, word_count_col_name):\n",
    "#     breakpoint()\n",
    "    main[word_count_col_name] = main[col_name].str.split().apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    return main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word(st):\n",
    "    words = st.split()\n",
    "    return len(words) if isinstance(words, list) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find_list_middle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_list_middle(input_list):\n",
    "    middle = float(len(input_list))/2\n",
    "    if middle % 2 != 0:\n",
    "        return int(middle + .5)\n",
    "    else:\n",
    "        return int(middle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fxn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print iterations progress\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█'):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6NbU1KA0-75",
    "toc-hr-collapsed": true
   },
   "source": [
    "## S Eng Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_df(df, feature_name):\n",
    "    feature_tokens = feature_name+'_tokens'\n",
    "    df[feature_tokens] = df[feature_name].apply(lambda x: sent_tokenize(str(x)))\n",
    "\n",
    "    df[feature_tokens] = df[feature_tokens].astype(str).str.lower()\n",
    "\n",
    "    df[feature_tokens] = df[feature_tokens].astype(str).str.replace(r'\\.\\!\\:\\?', \"\")\n",
    "\n",
    "    # split into words\n",
    "    df[feature_tokens] = df[feature_tokens].apply(lambda x: word_tokenize(str(x)))\n",
    "\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    df[feature_tokens] = df[feature_tokens].apply(lambda ls: [token for token in ls if len(token) > 1])\n",
    "\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df[feature_tokens] = df[feature_tokens].apply(lambda ls: [w for w in ls if not w in stop_words])\n",
    "\n",
    "    # stemming of words\n",
    "    porter = PorterStemmer()\n",
    "    df[feature_tokens] = df[feature_tokens].apply(lambda ls: [porter.stem(token) for token in ls])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzpbWRhzmtPn"
   },
   "source": [
    "### wrangle_for_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2abfwdn-g135"
   },
   "outputs": [],
   "source": [
    "def wrangle_for_keras(df, feature, bool_input, maxlen):\n",
    "    text = df[feature+'_idx'].values\n",
    "\n",
    "    # pad to max lenght of max_seq_length\n",
    "    tokens_padded = tf.keras.preprocessing.sequence.pad_sequences(text, maxlen=maxlen, padding='post')\n",
    "\n",
    "    # This is the test data to use with the bool_input layer\n",
    "#     bl = pd.get_dummies(df['pd_date']).as_matrix()\n",
    "    bl = pd.DataFrame(0, index=np.arange(df.shape[0]), columns=bool_input)\n",
    "#     bl = pd.concat([df, bd], axis=0)\n",
    "\n",
    "    print('Padded docs: ', tokens_padded[0:1])\n",
    "    print('padded docs shape: ', tokens_padded.shape)\n",
    "    return tokens_padded, bl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict_compliance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_compliance(df, tokens, boolean, k_model):\n",
    "    print('     Starting to classify the conformity')\n",
    "    compliance_model = keras.models.load_model(keras_classification_model_path)\n",
    "    compliance = compliance_model.predict([tokens,boolean], verbose=1)\n",
    "    # The model has 2 outputs in a list. The first one is only to train the lstm layer\n",
    "    # So it is discarded with the [1]\n",
    "    # \n",
    "    df['ai_se_comp'] = df['ai_se_comp'] = compliance[1][:,0]\n",
    "    # Only keeping the probability of compliance\n",
    "    print(f'          ...predicted {df.shape[0]} specs')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(df, keras_tokenizer_path, keras_classification_model_path):\n",
    "    df = df.dropna(subset=['spec'])\n",
    "    with open(keras_tokenizer_path, 'r') as infile:\n",
    "        json_tx = json.load(infile)\n",
    "    t = keras.preprocessing.text.tokenizer_from_json(json_tx)\n",
    "    k_model = tf.keras.models.load_model(keras_classification_model_path)\n",
    "    # Only 'para' cat row are classified\n",
    "    mask = df.cat.str.contains(r'para|list') & (df.word_nbr > 15)\n",
    "    docs = t.texts_to_sequences(df.loc[mask, 'spec_tokens'].tolist())\n",
    "#     breakpoint()    \n",
    "    docs = keras.preprocessing.sequence.pad_sequences(docs, maxlen=100, padding='post')\n",
    "    pred = k_model.predict(docs, verbose=True)\n",
    "    df.loc[mask, 'ai_se_compliance'] = pred\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(df.cat.str.contains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## SIF Formulas\n",
    "Using Gensim Word2Vec to create the embeddings\n",
    "\n",
    "Inspired from: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize_with_nltk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_with_nltk(df, feature):\n",
    "#     # TO-DO Should I build my own tokenizer?\n",
    "#     # TO-DO add lemmentization\n",
    "#     df.loc[:,:] = df.dropna(subset=[feature])\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     df[feature+'_tokens'] = df[feature].apply(lambda x: tokenizer.tokenize(str(x)))\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an embedding word with associated vector\n",
    "class Word:\n",
    "    def __init__(self, text, vector):\n",
    "        self.text = text\n",
    "        self.vector = vector\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.text + ' : ' + str(self.vector)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a sentence, a list of words\n",
    "class Sentence:\n",
    "    def __init__(self, word_list):\n",
    "        self.word_list = word_list\n",
    "\n",
    "    # return the length of a sentence\n",
    "    def len(self) -> int:\n",
    "        return len(self.word_list)\n",
    "\n",
    "    def __str__(self):\n",
    "        word_str_list = [word.text for word in self.word_list]\n",
    "        return ' '.join(word_str_list)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_word_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: get a proper word frequency for a word in a document set\n",
    "# or perhaps just a typical frequency for a word from Google's n-grams\n",
    "def get_word_frequency(word_text):\n",
    "    return 0.0001  # set to a low occurring frequency - probably not unrealistic for most words, improves vector values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence_to_vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A SIMPLE BUT TOUGH TO BEAT BASELINE FOR SENTENCE EMBEDDINGS\n",
    "# Sanjeev Arora, Yingyu Liang, Tengyu Ma\n",
    "# Princeton University\n",
    "# convert a list of sentence with word2vec items into a set of sentence vectors\n",
    "def sentence_to_vec(sentence_list: List[Sentence], embedding_size: int, a: float=1e-3):\n",
    "    sentence_set = []\n",
    "    sentence_set_id = []    \n",
    "    for dc in sentence_list:\n",
    "        sentence = dc['vec']\n",
    "        vs = np.zeros(embedding_size)  # add all word2vec values into one vector for the sentence\n",
    "        sentence_length = sentence.len()\n",
    "        for word in sentence.word_list:\n",
    "            a_value = a / (a + get_word_frequency(word.text))  # smooth inverse frequency, SIF\n",
    "            vs = np.add(vs, np.multiply(a_value, word.vector))  # vs += sif * word_vector\n",
    "\n",
    "        vs = np.divide(vs, sentence_length)  # weighted average\n",
    "        sentence_set.append(vs)  # add to our existing re-calculated set of sentences\n",
    "        sentence_set_id.append(dc['id'])\n",
    "\n",
    "    # calculate PCA of this sentence set\n",
    "#     if len(sentence_set) > 0:\n",
    "    pca = PCA()\n",
    "#     breakpoint()\n",
    "    pca.fit(np.array(sentence_set))\n",
    "    u = pca.components_[0]  # the PCA vector\n",
    "    u = np.multiply(u, np.transpose(u))  # u x uT\n",
    "\n",
    "    # pad the vector?  (occurs if we have less sentences than embeddings_size)\n",
    "    if len(u) < embedding_size:\n",
    "        for i in range(embedding_size - len(u)):\n",
    "            u = np.append(u, 0)  # add needed extension for multiplication below\n",
    "\n",
    "    # resulting sentence vectors, vs = vs -u x uT x vs\n",
    "    sentence_vecs = []\n",
    "    for i, vs in enumerate(sentence_set):\n",
    "        sub = np.multiply(u,vs)\n",
    "        sentence_vecs.append({'id':sentence_set_id[i], 'vec':np.subtract(vs, sub)})\n",
    "    return sentence_vecs\n",
    "\n",
    "#     return _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l2_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# euclidean distance between two vectors\n",
    "def l2_dist(v1, v2):\n",
    "    sum = 0.0\n",
    "    if len(v1) == len(v2):\n",
    "        for i in range(len(v1)):\n",
    "            delta = v1[i] - v2[i]\n",
    "            sum += delta * delta\n",
    "        return math.sqrt(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokens_2_word_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_2_word_vectors(df, feature, word2vec):\n",
    "    # convert the tokens to their vectors\n",
    "    sentence_list = []\n",
    "    for idx, row in df.iterrows():\n",
    "        word_list = []\n",
    "#         breakpoint()\n",
    "        for word in row[feature]:\n",
    "            if word in word2vec.wv.vocab:\n",
    "                token = word2vec.wv[word]\n",
    "                if len(token) > 1:  # ignore OOVs\n",
    "                    word_list.append(Word(word, token))\n",
    "                else:\n",
    "                    print('ERROROORORORORORO')\n",
    "        if len(word_list) > 0:  # did we find any words (not an empty set)\n",
    "            sentence_list.append({'id':idx, 'vec': Sentence(word_list)})\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build_sentence_vector_dc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sentence_vector_dc(sentence_list, embedding_size):\n",
    "    # apply single sentence word embedding\n",
    "#     sentence_vector_lookup = dict()\n",
    "    sentence_vectors = sentence_to_vec(sentence_list, embedding_size)  # all vectors converted together\n",
    "#     if len(sentence_vectors) == len(sentence_list): # There is an error if not\n",
    "#         for i in range(len(sentence_vectors)):\n",
    "#             # map: text of the sentence -> vector\n",
    "#             sentence_vector_lookup[sentence_list[i].__str__()] = sentence_vectors[i]\n",
    "    return sentence_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(df, embedding_size, word2vec_path, bank_path):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        fxn()\n",
    "\n",
    "        # Getting doc vectors for the current RFP specs\n",
    "        mask = df.cat.str.contains(r'para|list') & (df.word_nbr > 15)\n",
    "        print(f\"     Vectorizing {df[mask].shape[0]} bid specs...\")\n",
    "        word2vec = Word2Vec.load(word2vec_path)\n",
    "        bid_sentence_list = tokens_2_word_vectors(df[mask], 'spec_tokens', word2vec)\n",
    "        bid_sentence_vector_ls = build_sentence_vector_dc(bid_sentence_list, embedding_size)\n",
    "\n",
    "        # Getting doc vectors for the specs in the bank of historical compliance matrix\n",
    "        bank = pd.read_excel(bank_path)\n",
    "        bank['spec_tokens'] = np.nan\n",
    "        mask2 = bank.PAE.str.len() > 10\n",
    "        print(f\"     Vectorizing {bank[mask2].shape[0]} historical specs...\")\n",
    "        bank = tokenize_df(bank[mask2], \"spec\")\n",
    "        bank_sentence_list = tokens_2_word_vectors(bank[mask2], 'spec_tokens', word2vec)\n",
    "        bank_sentence_vector_ls = build_sentence_vector_dc(bank_sentence_list, embedding_size)\n",
    "        print(f\"Total of {bank[mask2].shape[0] * df[mask].shape[0]} vectorial combinations \")\n",
    "    return bid_sentence_list, bid_sentence_vector_ls, bank_sentence_list, bank_sentence_vector_ls, bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_eucledians()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eucledians(df, bank, bid_sentence_vector_ls, bank_sentence_vector_ls):\n",
    "    # Initial call to print 0% progress\n",
    "#     print(f'Number of specs to evaluate: {len(bid_sentence_vector_ls)}')\n",
    "    printProgressBar(0, len(bid_sentence_vector_ls), prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "    i = 0\n",
    "    for bid_dc in bid_sentence_vector_ls:\n",
    "    #     print()\n",
    "        best_euclidean_dist = 6\n",
    "        # initializing deque \n",
    "        best_match = collections.deque(maxlen=3)\n",
    "        # Update Progress Bar\n",
    "        printProgressBar(i + 1, len(bid_sentence_vector_ls), prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "\n",
    "        for bank_dc in bank_sentence_vector_ls:\n",
    "            # If we found the best\n",
    "            euclidean_dist = l2_dist(bid_dc['vec'], bank_dc['vec'])\n",
    "            if  euclidean_dist < best_euclidean_dist:\n",
    "                best_euclidean_dist = euclidean_dist\n",
    "                best_match.appendleft((bank_dc['id'], euclidean_dist))\n",
    "        i += 1\n",
    "        if len(best_match) > 0:\n",
    "            df.loc[bid_dc['id'], 'ai_pae_1'] = bank.loc[best_match[0][0],'PAE']\n",
    "            df.loc[bid_dc['id'], 'ai_pae_1_spec'] = bank.loc[best_match[0][0],'spec']\n",
    "            df.loc[bid_dc['id'], 'ai_pae_1_source'] = bank.loc[best_match[0][0],'Client']            \n",
    "            df.loc[bid_dc['id'], 'org_ai_pae_1_id'] = best_match[0][0]\n",
    "            df.loc[bid_dc['id'], 'org_ai_pae_1_score'] = best_match[0][1]\n",
    "        if len(best_match) > 1:\n",
    "            df.loc[bid_dc['id'], 'ai_pae_2'] = bank.loc[best_match[1][0],'PAE']\n",
    "            df.loc[bid_dc['id'], 'ai_pae_2_spec'] = bank.loc[best_match[1][0],'spec']\n",
    "            df.loc[bid_dc['id'], 'ai_pae_2_source'] = bank.loc[best_match[1][0],'Client']            \n",
    "            df.loc[bid_dc['id'], 'org_ai_pae_2_id'] = best_match[1][0]\n",
    "            df.loc[bid_dc['id'], 'org_ai_pae_2_score'] = best_match[1][1]\n",
    "        if len(best_match) > 2:\n",
    "            df.loc[bid_dc['id'], 'ai_pae_3'] = bank.loc[best_match[2][0],'PAE']\n",
    "            df.loc[bid_dc['id'], 'ai_pae_3_spec'] = bank.loc[best_match[2][0],'spec']\n",
    "            df.loc[bid_dc['id'], 'ai_pae_3_source'] = bank.loc[best_match[2][0],'Client']            \n",
    "            df.loc[bid_dc['id'], 'org_ai_pae_3_id'] = best_match[2][0]\n",
    "            df.loc[bid_dc['id'], 'org_ai_pae_3_score'] = best_match[2][1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove_poor_similarities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_poor_similarities(df):\n",
    "    # Small sentences should be filtered out even is similar score is good\n",
    "    mask = (df.org_ai_pae_1_score < 0.75) & (df.word_nbr > 10)\n",
    "    df.loc[~mask,'ai_pae_1'] = np.nan\n",
    "    df.loc[~mask,'ai_pae_1_spec'] = np.nan\n",
    "    df.loc[~mask,'ai_pae_1_source'] = np.nan\n",
    "\n",
    "    mask = (df.org_ai_pae_2_score < 0.75) & (df.word_nbr > 10)\n",
    "    df.loc[~mask,'ai_pae_2'] = np.nan\n",
    "    df.loc[~mask,'ai_pae_2_spec'] = np.nan\n",
    "    df.loc[~mask,'ai_pae_2_source'] = np.nan\n",
    "\n",
    "    mask = (df.org_ai_pae_3_score < 0.75) & (df.word_nbr > 10)\n",
    "    df.loc[~mask,'ai_pae_3'] = np.nan\n",
    "    df.loc[~mask,'ai_pae_3_spec'] = np.nan\n",
    "    df.loc[~mask,'ai_pae_3_source'] = np.nan\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt =remove_poor_similarities(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING REQUIRED FILES:\n",
      "     Created main dataframe.\n",
      "     Number of documents found: 1 document(s)\n",
      "     Processing document documents: [{'path': 'mississauga toc.xlsx'}]\n",
      "     Loading the RFP...\n",
      "     ...RFP loaded\n",
      "     Collecting RFP information...\n",
      "          ...Project is:  10000\n",
      "          ...Project Name is:  Mississauga\n",
      "\n",
      "PARSING THE TECHNICAL REQUIREMENTS:\n",
      "     Identifying lines and paragraphs...\n",
      "          ...Found 1375 paragraphs\n",
      "     Parsing text...\n",
      "          ...Parsed: 1375 entries\n",
      "     Identifying the headings...\n",
      "     Identifying lines and paragraphs...\n",
      "          ...Found 235 paragraphs\n",
      "     Parsing text...\n",
      "          ...Parsed: 235 entries\n",
      "     Parsing text...\n",
      "          ...Parsed: 235 entries\n",
      "     TOC contains 235 headings\n",
      "          ...Matched 228 headings from the main document against the TOC\n",
      "     Identifiying lists item...\n",
      "          ...Found 248 list items\n",
      "     Identifiying tables...\n",
      "          ...Found 27 table rows\n",
      "     Correcting pages breaks...\n",
      "          ...Number of pages breaks corrected: 169 in 3.0281923714759538 loop(s)\n",
      "     Looking for heading not captured by TOC...\n",
      "     ... found 29 headings not captured by TOC.\n",
      "     Current document has 1206 rows. Now spliting long paragraphs...\n",
      "          ...Splited 191 paragraphs over 4 document cycles. Document has now 1397 rows. \n",
      "     Removing unwanted text (i.e. page numbers in the footers)...\n",
      "          ...Rejected 0 entries.\n",
      "     Creating specs ids...\n",
      "          ...Created 1397 ids.\n",
      "     Collecting RFP information...\n",
      "          ...Project is:  10000\n",
      "          ...Project Name is:  Mississauga\n"
     ]
    }
   ],
   "source": [
    "# time.sleep(20)\n",
    "\n",
    "# Transactional formulas\n",
    "print('LOADING REQUIRED FILES:')\n",
    "df, column_name_ls = create_standard_df() # empty df\n",
    "meta, toc, tech = get_rfp(rfp_path)\n",
    "df, bid_number, bid_name, spec_first_col = add_meta(df, meta)\n",
    "\n",
    "# Parsing\n",
    "print()\n",
    "print('PARSING THE TECHNICAL REQUIREMENTS:')\n",
    "df = parse_raw_row_column(df,'spec', tech, spec_first_col)\n",
    "df = parse_string(df, 'spec')\n",
    "df = count_se_word(df, 'spec', 'word_nbr') # Need word count value to properly select the heading in parst_toc()\n",
    "df = parse_toc(df, toc)\n",
    "\n",
    "df = parse_list(df)\n",
    "\n",
    "df = parse_table(df, 'spec', spec_first_col)\n",
    "\n",
    "# if spec_first_col == 0:\n",
    "df = parse_page(df)\n",
    "df = parse_heading(df)\n",
    "df = parse_paragraph(df)\n",
    "df = count_se_word(df, 'spec', 'word_nbr')\n",
    "df, rejected = parse_out_text(df)\n",
    "\n",
    "# Feature Management\n",
    "df = add_id(df)\n",
    "df, bid_number, bid_name, spec_first_col = add_meta(df, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEXT PREPROCESSING\n",
      "     Tokenize specs...\n",
      "    ...Filtered stop words.\n",
      "    ...Stemmed word.\n",
      "\n",
      "CLASSIFYING COMPLIANCE WITH AI:\n",
      "839/839 [==============================] - 1s 893us/sample\n",
      "\n",
      "PAE SUGGESTIONS:\n",
      "     Vectorizing 839 bid specs...\n",
      "     Vectorizing 17452 historical specs...\n",
      "Total of 14642228 vectorial combinations \n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('TEXT PREPROCESSING')\n",
    "df = tokenize(df)\n",
    "\n",
    "print()\n",
    "print('CLASSIFYING COMPLIANCE WITH AI:')\n",
    "df = run_classifier(df, keras_tokenizer_path, keras_classification_model_path)\n",
    "\n",
    "print()\n",
    "print('PAE SUGGESTIONS:')\n",
    "bid_sentence_list, bid_sentence_vector_ls, bank_sentence_list, bank_sentence_vector_ls, bank = generate_embeddings(df, embedding_size, word2vec_path, bank_path)\n",
    "df = generate_eucledians(df, bank, bid_sentence_vector_ls, bank_sentence_vector_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FORMATING THE OUTPUTS:\n"
     ]
    }
   ],
   "source": [
    "df = remove_poor_similarities(df)\n",
    "\n",
    "print()\n",
    "print('FORMATING THE OUTPUTS:')\n",
    "df = formating_out_tc(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document successfuly created!\n"
     ]
    }
   ],
   "source": [
    "df = formating_heading(df)\n",
    "df = formating_out_cid(df)\n",
    "\n",
    "df = count_se_word(df, 'spec_one_col', 'word_nbr')\n",
    "\n",
    "_ = saving_df(df, rejected, column_name_ls, output_path, '190927')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df.spec.str.contains('Replacement vehicles shall re-use equipment as specified in Section 9'),'spec']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('cat').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compliance Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Example of compliance predictions:')\n",
    "for isx, row in df.sample(n=5).iterrows():\n",
    "    print()\n",
    "    print(f'Compliance: ', row.ai_se_compliance)\n",
    "    print(f'Spec: ', row.spec)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokens examples: ',bank.spec_tokens[1120:1124])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_classifier(df, keras_tokenizer_path, keras_classification_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank.info()\n",
    "desc_ls = bank.Description.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(ls, tokens_only=False):\n",
    "    for i, line in enumerate(desc_ls):\n",
    "        if tokens_only:\n",
    "            yield gensim.utils.simple_preprocess(line)\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(descriptions))\n",
    "# test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time doc2vec.train(train_corpus, total_examples=doc2vec.corpus_count, epochs=doc2vec.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_sim = []\n",
    "# second_doc = []\n",
    "for i in range(50):\n",
    "    doc =  random.choice(train_corpus)\n",
    "\n",
    "#     first_doc.append(doc)\n",
    "    inferred_vector = doc2vec.infer_vector(doc.words)\n",
    "    sim = doc2vec.docvecs.most_similar([inferred_vector], topn=3)\n",
    "#     breakpoint()\n",
    "    most_sim.append({'spec':doc.tags[0], 'sim':sim})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in most_sim:\n",
    "#     x =  random.choice(most_sim)\n",
    "#     breakpoint()\n",
    "    print()\n",
    "    print(f'MAIN  : {train_corpus[x[\"spec\"]]}')\n",
    "    print(f'MATCH : {x[\"sim\"][0][1]}')\n",
    "    print(f'SIM   : {train_corpus[x[\"sim\"][0][0]]}')\n",
    "    \n",
    "# len(most_sim)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gensim.models.doc2vec.TaggedDocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(SentenceTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"C:\\\\Users\\\\a324448\\\\\\.cache\\\\torch\\\\sentence_transformers\\\\public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\\\\modules.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train model with Gensim Word2Vec\n",
    "# model = Word2Vec(descriptions, size=300, min_count=1)\n",
    "# # summarize the loaded model\n",
    "# print(model)\n",
    "word2vec = Word2Vec.load(word2vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize vocabulary\n",
    "words = list(word2vec.wv.vocab)\n",
    "print('word examples in the vocab: ',words[100:110])\n",
    "# access vector for one word\n",
    "print('First representations of the vector: ', word2vec['moisture'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.wv.most_similar('door')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(word2vec.wv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
